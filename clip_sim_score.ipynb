{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "ds = load_dataset(\"adhamelarabawy/fashion_human_classification\").shuffle()[\"train\"]\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "samples = ds.shuffle().select(range(num_samples))\n",
    "\n",
    "def add_score(sample):\n",
    "    image = preprocess(sample[\"image\"]).unsqueeze(0)\n",
    "    text = tokenizer([\"human\"])\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        text_probs = (100.0 * image_features @ text_features.T)# .softmax(dim=-1)\n",
    "    sample[\"score\"] = text_probs.item()\n",
    "    return sample\n",
    "\n",
    "samples = samples.map(add_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of scores for the samples, categorized by label\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "labels = np.array(samples[\"has_human\"])\n",
    "scores = np.array(samples[\"score\"])\n",
    "\n",
    "# fig = plt.hist(scores[labels==False], bins=100)\n",
    "plt.hist(scores[labels==False], bins=100, alpha=0.5, label='No Human')\n",
    "plt.hist(scores[labels==True], bins=100, alpha=0.5, label='Human')\n",
    "\n",
    "plt.xlabel('CLIP Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('CLIP Similarity Scores for Human vs. No Human Images (n=1000, ViT-g-14)')\n",
    "plt.legend()\n",
    "\n",
    "thresh = ((scores[labels==True].mean()) + (scores[labels==False].mean())) / 2\n",
    "thresh = 4.5\n",
    "\n",
    "# draw vertical line at thresh\n",
    "plt.axvline(x=thresh, color='k', linestyle='--')\n",
    "# draw vercial line at each mean\n",
    "plt.axvline(x=scores[labels==True].mean(), color='gray', linestyle='--')\n",
    "plt.axvline(x=scores[labels==False].mean(), color='gray', linestyle='--')\n",
    "\n",
    "tp = np.sum(scores[labels==True] > thresh)\n",
    "tn = np.sum(scores[labels==False] < thresh)\n",
    "fp = np.sum(scores[labels==False] > thresh)\n",
    "fn = np.sum(scores[labels==True] < thresh)\n",
    "\n",
    "print(f\"Accuracy: {(tp+tn)/(tp+tn+fp+fn)}\")\n",
    "print(f\"Precision: {tp/(tp+fp)}\")\n",
    "print(f\"Recall: {tp/(tp+fn)}\")\n",
    "print(f\"F1: {2*tp/(2*tp+fp+fn)}\")\n",
    "print(f\"Keep Rate: {1 - len(scores[labels==True] > thresh)/len(scores)}\")\n",
    "# plt.savefig(\"results/vanilla_scores.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
